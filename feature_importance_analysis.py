"""
============================================================
ë§¨ì¦ˆì¼€ì–´ ë°ì´ì•¤ë‚˜ì´íŠ¸ ë“€ì–¼ ìƒ´í‘¸ - Feature Importance ë¶„ì„
============================================================
êµ¬ë§¤ ì˜í–¥ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ë³€ìˆ˜ ì¶”ì¶œ

ëª¨ë¸:
1. Logistic Regression - ë³€ìˆ˜ë³„ ì˜í–¥ ë°©í–¥(+/-) ë¶„ì„
2. Decision Tree - êµ¬ë§¤ ê·œì¹™ ë„ì¶œ
3. Random Forest - ì•ˆì •ì ì¸ ì¤‘ìš”ë„ ìˆœìœ„
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

# CSV íŒŒì¼ ì½ê¸°
df = pd.read_csv('í—¤ì–´Â·ë‘í”¼ ì¼€ì–´ ì œí’ˆì— ëŒ€í•œ ìˆ˜ìš” ì„¤ë¬¸ì¡°ì‚¬(ì‘ë‹µ) - ì„¤ë¬¸ì§€ ì‘ë‹µ ì‹œíŠ¸1.csv', skiprows=5, header=None)
df.columns = ['íƒ€ì„ìŠ¤íƒ¬í”„', 'ì„±ë³„', 'ì—°ë ¹ëŒ€', 'ë¨¸ë¦¬ê°ëŠ”ì‹œê°„', 'ë‘í”¼ê³ ë¯¼', 'ìƒ´í‘¸ì„ íƒì´ìœ ', 'ìƒ´í‘¸ì•„ì‰¬ìš´ì ', 'Q7', 'Q8', 'ê¸°íƒ€1', 'ê¸°íƒ€2']

print("=" * 70)
print("ğŸ”¬ Feature Importance ë¶„ì„ - êµ¬ë§¤ ì˜í–¥ ì˜í–¥ ìš”ì¸")
print("=" * 70)

# ============================================================
# ë°ì´í„° ì „ì²˜ë¦¬
# ============================================================
print("\nğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")

# íƒ€ê²Ÿ ë³€ìˆ˜ ì¸ì½”ë”©
df['êµ¬ë§¤ì˜í–¥'] = (df['Q8'] == 'ìˆë‹¤').astype(int)

# í”¼ì²˜ ì¸ì½”ë”©
le_gender = LabelEncoder()
le_age = LabelEncoder()
le_time = LabelEncoder()

df['ì„±ë³„_encoded'] = le_gender.fit_transform(df['ì„±ë³„'])
df['ì—°ë ¹ëŒ€_encoded'] = le_age.fit_transform(df['ì—°ë ¹ëŒ€'])
df['ë¨¸ë¦¬ê°ëŠ”ì‹œê°„_encoded'] = le_time.fit_transform(df['ë¨¸ë¦¬ê°ëŠ”ì‹œê°„'])
df['Q7_score'] = pd.to_numeric(df['Q7'], errors='coerce').fillna(3)

# í•˜ë£¨ 2ë²ˆ ìƒ´í‘¸ ì—¬ë¶€
df['í•˜ë£¨2ë²ˆìƒ´í‘¸'] = df['ë¨¸ë¦¬ê°ëŠ”ì‹œê°„'].str.contains('ì•„ì¹¨&ì €ë…', na=False).astype(int)

# ë‘í”¼ ê³ ë¯¼ One-hot encoding
scalp_concerns = ['ë‘í”¼ ì—´ê°', 'ìœ ë¶„ ê³¼ë‹¤', 'ê±´ì¡°í•¨', 'ê°€ë ¤ì›€', 'íƒˆëª¨', 'ë¯¼ê°ì„±', 'íŠ¹ë³„í•œ ê³ ë¯¼ ì—†ìŒ']
for concern in scalp_concerns:
    df[f'ê³ ë¯¼_{concern}'] = df['ë‘í”¼ê³ ë¯¼'].str.contains(concern, na=False).astype(int)

# ìƒ´í‘¸ ì„ íƒ ì´ìœ  One-hot encoding
shampoo_reasons = ['ë‘í”¼ ì¼€ì–´', 'íƒˆëª¨ ì™„í™”', 'ì„¸ì •ë ¥', 'í–¥', 'ê°€ê²©', 'ë¸Œëœë“œ']
for reason in shampoo_reasons:
    df[f'ì´ìœ _{reason}'] = df['ìƒ´í‘¸ì„ íƒì´ìœ '].str.contains(reason, na=False).astype(int)

# í”¼ì²˜ ì„ íƒ
feature_names = ['ì„±ë³„_encoded', 'ì—°ë ¹ëŒ€_encoded', 'Q7_score', 'í•˜ë£¨2ë²ˆìƒ´í‘¸'] + \
                [f'ê³ ë¯¼_{c}' for c in scalp_concerns] + \
                [f'ì´ìœ _{r}' for r in shampoo_reasons]

# í”¼ì²˜ëª… í•œê¸€í™” (ì‹œê°í™”ìš©)
feature_names_kr = ['ì„±ë³„', 'ì—°ë ¹ëŒ€', 'Q7(ë‘í”¼ì°¨ì´ì¸ì‹)', 'í•˜ë£¨2ë²ˆìƒ´í‘¸',
                    'ê³ ë¯¼:ë‘í”¼ì—´ê°', 'ê³ ë¯¼:ìœ ë¶„ê³¼ë‹¤', 'ê³ ë¯¼:ê±´ì¡°í•¨', 'ê³ ë¯¼:ê°€ë ¤ì›€', 
                    'ê³ ë¯¼:íƒˆëª¨', 'ê³ ë¯¼:ë¯¼ê°ì„±', 'ê³ ë¯¼:ì—†ìŒ',
                    'ì´ìœ :ë‘í”¼ì¼€ì–´', 'ì´ìœ :íƒˆëª¨ì™„í™”', 'ì´ìœ :ì„¸ì •ë ¥', 
                    'ì´ìœ :í–¥', 'ì´ìœ :ê°€ê²©', 'ì´ìœ :ë¸Œëœë“œ']

X = df[feature_names]
y = df['êµ¬ë§¤ì˜í–¥']

print(f"   ì „ì²´ ìƒ˜í”Œ ìˆ˜: {len(df)}ëª…")
print(f"   í”¼ì²˜ ìˆ˜: {len(feature_names)}ê°œ")
print(f"   êµ¬ë§¤ ì˜í–¥ ìˆìŒ: {y.sum()}ëª… ({y.mean()*100:.1f}%)")
print(f"   êµ¬ë§¤ ì˜í–¥ ì—†ìŒ: {len(y) - y.sum()}ëª… ({(1-y.mean())*100:.1f}%)")

# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)

# ìŠ¤ì¼€ì¼ë§ (Logistic Regressionìš©)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ============================================================
# 1. Logistic Regression ë¶„ì„
# ============================================================
print("\n" + "=" * 70)
print("ğŸ“Œ 1. Logistic Regression - ë³€ìˆ˜ë³„ ì˜í–¥ ë°©í–¥ ë¶„ì„")
print("=" * 70)

lr_model = LogisticRegression(random_state=42, max_iter=1000)
lr_model.fit(X_train_scaled, y_train)

# ì˜ˆì¸¡ ë° í‰ê°€
y_pred_lr = lr_model.predict(X_test_scaled)
y_prob_lr = lr_model.predict_proba(X_test_scaled)[:, 1]
accuracy_lr = accuracy_score(y_test, y_pred_lr)

print(f"\nëª¨ë¸ ì •í™•ë„: {accuracy_lr:.1%}")

# ê³„ìˆ˜ ë¶„ì„
lr_coef = pd.DataFrame({
    'í”¼ì²˜': feature_names_kr,
    'ê³„ìˆ˜': lr_model.coef_[0],
    'ì˜í–¥ë°©í–¥': ['ê¸ì •(+)' if c > 0 else 'ë¶€ì •(-)' for c in lr_model.coef_[0]],
    'ì ˆëŒ€ê°’': np.abs(lr_model.coef_[0])
}).sort_values('ì ˆëŒ€ê°’', ascending=False)

print("\nğŸ“Š Logistic Regression ê³„ìˆ˜ (êµ¬ë§¤ ì˜í–¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥):")
print("-" * 60)
for idx, row in lr_coef.head(10).iterrows():
    direction = "ğŸ”º" if row['ê³„ìˆ˜'] > 0 else "ğŸ”»"
    bar = "â–ˆ" * int(row['ì ˆëŒ€ê°’'] * 5)
    print(f"   {direction} {row['í”¼ì²˜']:20s}: {row['ê³„ìˆ˜']:+.3f} {bar}")

print("\nğŸ’¡ í•´ì„:")
top_positive = lr_coef[lr_coef['ê³„ìˆ˜'] > 0].head(3)
top_negative = lr_coef[lr_coef['ê³„ìˆ˜'] < 0].head(3)
print("   êµ¬ë§¤ ì˜í–¥ì„ ë†’ì´ëŠ” ìš”ì¸:")
for _, row in top_positive.iterrows():
    print(f"      â€¢ {row['í”¼ì²˜']}")
print("   êµ¬ë§¤ ì˜í–¥ì„ ë‚®ì¶”ëŠ” ìš”ì¸:")
for _, row in top_negative.iterrows():
    print(f"      â€¢ {row['í”¼ì²˜']}")

# ============================================================
# 2. Decision Tree ë¶„ì„
# ============================================================
print("\n" + "=" * 70)
print("ğŸ“Œ 2. Decision Tree - êµ¬ë§¤ ê·œì¹™ ë„ì¶œ")
print("=" * 70)

dt_model = DecisionTreeClassifier(max_depth=4, min_samples_leaf=5, random_state=42)
dt_model.fit(X_train, y_train)

y_pred_dt = dt_model.predict(X_test)
accuracy_dt = accuracy_score(y_test, y_pred_dt)

print(f"\nëª¨ë¸ ì •í™•ë„: {accuracy_dt:.1%}")

# Feature Importance
dt_importance = pd.DataFrame({
    'í”¼ì²˜': feature_names_kr,
    'ì¤‘ìš”ë„': dt_model.feature_importances_
}).sort_values('ì¤‘ìš”ë„', ascending=False)

print("\nğŸ“Š Decision Tree Feature Importance:")
print("-" * 60)
for idx, row in dt_importance[dt_importance['ì¤‘ìš”ë„'] > 0].iterrows():
    bar = "â–ˆ" * int(row['ì¤‘ìš”ë„'] * 30)
    print(f"   {row['í”¼ì²˜']:20s}: {row['ì¤‘ìš”ë„']:.3f} {bar}")

# ============================================================
# 3. Random Forest ë¶„ì„
# ============================================================
print("\n" + "=" * 70)
print("ğŸ“Œ 3. Random Forest - ì•ˆì •ì ì¸ ì¤‘ìš”ë„ ìˆœìœ„")
print("=" * 70)

rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
rf_model.fit(X_train, y_train)

y_pred_rf = rf_model.predict(X_test)
y_prob_rf = rf_model.predict_proba(X_test)[:, 1]
accuracy_rf = accuracy_score(y_test, y_pred_rf)

# Cross-validation
cv_scores = cross_val_score(rf_model, X, y, cv=5)

print(f"\nëª¨ë¸ ì •í™•ë„: {accuracy_rf:.1%}")
print(f"êµì°¨ ê²€ì¦ ì •í™•ë„: {cv_scores.mean():.1%} (Â±{cv_scores.std()*2:.1%})")

# Feature Importance
rf_importance = pd.DataFrame({
    'í”¼ì²˜': feature_names_kr,
    'ì¤‘ìš”ë„': rf_model.feature_importances_
}).sort_values('ì¤‘ìš”ë„', ascending=False)

print("\nğŸ“Š Random Forest Feature Importance (Top 10):")
print("-" * 60)
for idx, row in rf_importance.head(10).iterrows():
    bar = "â–ˆ" * int(row['ì¤‘ìš”ë„'] * 50)
    print(f"   {row['í”¼ì²˜']:20s}: {row['ì¤‘ìš”ë„']:.3f} {bar}")

# ============================================================
# 4. ì¢…í•© ë¶„ì„ ê²°ê³¼
# ============================================================
print("\n" + "=" * 70)
print("ğŸ“Œ 4. ì¢…í•© ë¶„ì„ ê²°ê³¼")
print("=" * 70)

# ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:")
print("-" * 40)
print(f"   Logistic Regression: {accuracy_lr:.1%}")
print(f"   Decision Tree:       {accuracy_dt:.1%}")
print(f"   Random Forest:       {accuracy_rf:.1%}")

# ì„¸ ëª¨ë¸ì˜ Top 5 í”¼ì²˜ ì¢…í•©
print("\nğŸ“Š ëª¨ë¸ë³„ Top 5 ì¤‘ìš” í”¼ì²˜:")
print("-" * 60)
print(f"{'ìˆœìœ„':<6}{'Logistic Reg.':<20}{'Decision Tree':<20}{'Random Forest':<20}")
print("-" * 60)
for i in range(5):
    lr_feat = lr_coef.iloc[i]['í”¼ì²˜']
    dt_feat = dt_importance.iloc[i]['í”¼ì²˜'] if i < len(dt_importance[dt_importance['ì¤‘ìš”ë„'] > 0]) else "-"
    rf_feat = rf_importance.iloc[i]['í”¼ì²˜']
    print(f"{i+1:<6}{lr_feat:<20}{dt_feat:<20}{rf_feat:<20}")

# ê³µí†µ ì¤‘ìš” í”¼ì²˜ ì°¾ê¸°
top5_lr = set(lr_coef.head(5)['í”¼ì²˜'])
top5_dt = set(dt_importance.head(5)['í”¼ì²˜'])
top5_rf = set(rf_importance.head(5)['í”¼ì²˜'])
common_features = top5_lr & top5_rf

print(f"\nğŸ’¡ 3ê°œ ëª¨ë¸ì—ì„œ ê³µí†µìœ¼ë¡œ ì¤‘ìš”í•œ í”¼ì²˜:")
for feat in common_features:
    print(f"   âœ… {feat}")

# ============================================================
# 5. ê¸°íš ì¸ì‚¬ì´íŠ¸
# ============================================================
print("\n" + "=" * 70)
print("ğŸ“Œ 5. ë°ì´ì•¤ë‚˜ì´íŠ¸ ë“€ì–¼ ìƒ´í‘¸ ê¸°íš ì¸ì‚¬ì´íŠ¸")
print("=" * 70)

print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Feature Importance ë¶„ì„ ê²°ê³¼ ìš”ì•½                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚""")

# Q7 ì ìˆ˜ ì¤‘ìš”ë„ í™•ì¸
q7_rank_rf = rf_importance[rf_importance['í”¼ì²˜'] == 'Q7(ë‘í”¼ì°¨ì´ì¸ì‹)'].index[0] + 1
q7_importance = rf_importance[rf_importance['í”¼ì²˜'] == 'Q7(ë‘í”¼ì°¨ì´ì¸ì‹)']['ì¤‘ìš”ë„'].values[0]

print(f"""â”‚  1ï¸âƒ£  Q7(ì•„ì¹¨/ë°¤ ë‘í”¼ ì°¨ì´ ì¸ì‹)                                     â”‚
â”‚      â€¢ Random Forest ì¤‘ìš”ë„ ìˆœìœ„: {q7_rank_rf}ìœ„                       â”‚
â”‚      â€¢ ì¤‘ìš”ë„ ì ìˆ˜: {q7_importance:.3f}                                â”‚
â”‚      â†’ ì œí’ˆ ì»¨ì…‰ì˜ í•µì‹¬ ê·¼ê±°!                                         â”‚""")

# í•˜ë£¨ 2ë²ˆ ìƒ´í‘¸ ì¤‘ìš”ë„
twice_rank = rf_importance[rf_importance['í”¼ì²˜'] == 'í•˜ë£¨2ë²ˆìƒ´í‘¸'].index[0] + 1
twice_importance = rf_importance[rf_importance['í”¼ì²˜'] == 'í•˜ë£¨2ë²ˆìƒ´í‘¸']['ì¤‘ìš”ë„'].values[0]

print(f"""â”‚                                                                     â”‚
â”‚  2ï¸âƒ£  í•˜ë£¨ 2ë²ˆ ìƒ´í‘¸ ì—¬ë¶€                                              â”‚
â”‚      â€¢ Random Forest ì¤‘ìš”ë„ ìˆœìœ„: {twice_rank}ìœ„                       â”‚
â”‚      â€¢ ì¤‘ìš”ë„ ì ìˆ˜: {twice_importance:.3f}                             â”‚
â”‚      â†’ í•µì‹¬ íƒ€ê²Ÿ ê³ ê° ì„ ì • ê·¼ê±°!                                      â”‚""")

# íƒˆëª¨ ê³ ë¯¼ ì¤‘ìš”ë„
hairloss_importance = rf_importance[rf_importance['í”¼ì²˜'] == 'ê³ ë¯¼:íƒˆëª¨']['ì¤‘ìš”ë„'].values[0]

print(f"""â”‚                                                                     â”‚
â”‚  3ï¸âƒ£  íƒˆëª¨ ê³ ë¯¼                                                       â”‚
â”‚      â€¢ ì¤‘ìš”ë„ ì ìˆ˜: {hairloss_importance:.3f}                          â”‚
â”‚      â†’ ë‚˜ì´íŠ¸ ìƒ´í‘¸ íƒˆëª¨ ì™„í™” ê¸°ëŠ¥ ê·¼ê±°!                               â”‚""")

print("""â”‚                                                                     â”‚
â”‚  âœ… ê²°ë¡ :                                                            â”‚
â”‚     "ì•„ì¹¨/ë°¤ ë‘í”¼ ìƒíƒœ ì°¨ì´ë¥¼ ì¸ì‹í•˜ëŠ” ê³ ê°"ì´                        â”‚
â”‚     êµ¬ë§¤ ì˜í–¥ì´ ë†’ë‹¤ëŠ” ê²ƒì´ ë°ì´í„°ë¡œ ê²€ì¦ë¨                           â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

# ============================================================
# ì‹œê°í™” ìƒì„±
# ============================================================
print("=" * 70)
print("ğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
print("=" * 70)

# Figure 1: Feature Importance ë¹„êµ (3ê°œ ëª¨ë¸)
fig1, axes = plt.subplots(1, 3, figsize=(18, 8))

# Logistic Regression ê³„ìˆ˜
ax1 = axes[0]
colors1 = ['#2ecc71' if c > 0 else '#e74c3c' for c in lr_coef.head(10)['ê³„ìˆ˜']]
bars1 = ax1.barh(lr_coef.head(10)['í”¼ì²˜'], lr_coef.head(10)['ì ˆëŒ€ê°’'], color=colors1)
ax1.set_xlabel('ê³„ìˆ˜ ì ˆëŒ€ê°’', fontsize=12)
ax1.set_title('Logistic Regression\n(ğŸŸ¢ ê¸ì •ì  / ğŸ”´ ë¶€ì •ì  ì˜í–¥)', fontsize=14, fontweight='bold')
ax1.invert_yaxis()

# Decision Tree Feature Importance
ax2 = axes[1]
dt_top10 = dt_importance.head(10)
bars2 = ax2.barh(dt_top10['í”¼ì²˜'], dt_top10['ì¤‘ìš”ë„'], color='#3498db')
ax2.set_xlabel('Feature Importance', fontsize=12)
ax2.set_title('Decision Tree\nFeature Importance', fontsize=14, fontweight='bold')
ax2.invert_yaxis()

# Random Forest Feature Importance
ax3 = axes[2]
rf_top10 = rf_importance.head(10)
bars3 = ax3.barh(rf_top10['í”¼ì²˜'], rf_top10['ì¤‘ìš”ë„'], color='#9b59b6')
ax3.set_xlabel('Feature Importance', fontsize=12)
ax3.set_title('Random Forest\nFeature Importance', fontsize=14, fontweight='bold')
ax3.invert_yaxis()

plt.suptitle('êµ¬ë§¤ ì˜í–¥ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì¸ ë¶„ì„', fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('feature_importance_comparison.png', dpi=150, bbox_inches='tight', facecolor='white')
print("   âœ… feature_importance_comparison.png ì €ì¥ ì™„ë£Œ")

# Figure 2: Decision Tree ì‹œê°í™”
fig2, ax = plt.subplots(figsize=(20, 12))
plot_tree(dt_model, 
          feature_names=feature_names_kr, 
          class_names=['êµ¬ë§¤ì˜í–¥ì—†ìŒ', 'êµ¬ë§¤ì˜í–¥ìˆìŒ'],
          filled=True, 
          rounded=True,
          fontsize=9,
          ax=ax)
plt.title('Decision Tree - êµ¬ë§¤ ì˜í–¥ ì˜ˆì¸¡ ê·œì¹™', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig('decision_tree_visualization.png', dpi=150, bbox_inches='tight', facecolor='white')
print("   âœ… decision_tree_visualization.png ì €ì¥ ì™„ë£Œ")

# Figure 3: ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ + ROC Curve
fig3, axes = plt.subplots(1, 2, figsize=(14, 5))

# ëª¨ë¸ ì •í™•ë„ ë¹„êµ
ax1 = axes[0]
models = ['Logistic\nRegression', 'Decision\nTree', 'Random\nForest']
accuracies = [accuracy_lr, accuracy_dt, accuracy_rf]
colors = ['#2ecc71', '#3498db', '#9b59b6']
bars = ax1.bar(models, accuracies, color=colors, edgecolor='black', linewidth=1.5)
ax1.set_ylim(0, 1)
ax1.set_ylabel('ì •í™•ë„ (Accuracy)', fontsize=12)
ax1.set_title('ëª¨ë¸ë³„ ì˜ˆì¸¡ ì •í™•ë„', fontsize=14, fontweight='bold')
for bar, acc in zip(bars, accuracies):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, 
             f'{acc:.1%}', ha='center', va='bottom', fontsize=12, fontweight='bold')

# ROC Curve
ax2 = axes[1]
fpr_lr, tpr_lr, _ = roc_curve(y_test, y_prob_lr)
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)
auc_lr = roc_auc_score(y_test, y_prob_lr)
auc_rf = roc_auc_score(y_test, y_prob_rf)

ax2.plot(fpr_lr, tpr_lr, color='#2ecc71', lw=2, label=f'Logistic Regression (AUC = {auc_lr:.3f})')
ax2.plot(fpr_rf, tpr_rf, color='#9b59b6', lw=2, label=f'Random Forest (AUC = {auc_rf:.3f})')
ax2.plot([0, 1], [0, 1], 'k--', lw=1)
ax2.set_xlim([0, 1])
ax2.set_ylim([0, 1.05])
ax2.set_xlabel('False Positive Rate', fontsize=12)
ax2.set_ylabel('True Positive Rate', fontsize=12)
ax2.set_title('ROC Curve', fontsize=14, fontweight='bold')
ax2.legend(loc='lower right')

plt.tight_layout()
plt.savefig('model_performance.png', dpi=150, bbox_inches='tight', facecolor='white')
print("   âœ… model_performance.png ì €ì¥ ì™„ë£Œ")

# Figure 4: ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ìš”ì•½ ì‹œê°í™”
fig4, axes = plt.subplots(2, 2, figsize=(14, 12))

# 4-1: Q7 ì ìˆ˜ë³„ êµ¬ë§¤ ì˜í–¥
ax1 = axes[0, 0]
q7_purchase = df.groupby('Q7_score')['êµ¬ë§¤ì˜í–¥'].agg(['mean', 'count']).reset_index()
q7_purchase = q7_purchase[q7_purchase['Q7_score'].between(1, 5)]
colors = plt.cm.RdYlGn(q7_purchase['mean'])
bars = ax1.bar(q7_purchase['Q7_score'].astype(int).astype(str), q7_purchase['mean'], color=colors, edgecolor='black')
ax1.set_xlabel('Q7 ì ìˆ˜ (ì•„ì¹¨/ë°¤ ë‘í”¼ ì°¨ì´ ì¸ì‹)', fontsize=12)
ax1.set_ylabel('êµ¬ë§¤ ì˜í–¥ ë¹„ìœ¨', fontsize=12)
ax1.set_title('Q7 ì ìˆ˜ë³„ êµ¬ë§¤ ì˜í–¥ ë¹„ìœ¨', fontsize=14, fontweight='bold')
ax1.set_ylim(0, 1)
for bar, (_, row) in zip(bars, q7_purchase.iterrows()):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, 
             f'{row["mean"]:.0%}\n(n={int(row["count"])})', ha='center', va='bottom', fontsize=10)

# 4-2: ë¨¸ë¦¬ ê°ëŠ” ì‹œê°„ëŒ€ë³„ êµ¬ë§¤ ì˜í–¥
ax2 = axes[0, 1]
time_purchase = df.groupby('ë¨¸ë¦¬ê°ëŠ”ì‹œê°„')['êµ¬ë§¤ì˜í–¥'].agg(['mean', 'count']).reset_index()
time_purchase = time_purchase.sort_values('mean', ascending=True)
colors = plt.cm.Blues(np.linspace(0.3, 0.9, len(time_purchase)))
bars = ax2.barh(time_purchase['ë¨¸ë¦¬ê°ëŠ”ì‹œê°„'], time_purchase['mean'], color=colors, edgecolor='black')
ax2.set_xlabel('êµ¬ë§¤ ì˜í–¥ ë¹„ìœ¨', fontsize=12)
ax2.set_title('ë¨¸ë¦¬ ê°ëŠ” ì‹œê°„ëŒ€ë³„ êµ¬ë§¤ ì˜í–¥', fontsize=14, fontweight='bold')
ax2.set_xlim(0, 1)
for bar, (_, row) in zip(bars, time_purchase.iterrows()):
    ax2.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height()/2, 
             f'{row["mean"]:.0%} (n={int(row["count"])})', ha='left', va='center', fontsize=10)

# 4-3: ë‘í”¼ ê³ ë¯¼ë³„ êµ¬ë§¤ ì˜í–¥
ax3 = axes[1, 0]
concern_purchase = []
for concern in scalp_concerns[:-1]:  # íŠ¹ë³„í•œ ê³ ë¯¼ ì—†ìŒ ì œì™¸
    concern_df = df[df[f'ê³ ë¯¼_{concern}'] == 1]
    if len(concern_df) >= 5:
        concern_purchase.append({
            'ê³ ë¯¼': concern,
            'êµ¬ë§¤ì˜í–¥': concern_df['êµ¬ë§¤ì˜í–¥'].mean(),
            'ì‘ë‹µìˆ˜': len(concern_df)
        })
concern_df = pd.DataFrame(concern_purchase).sort_values('êµ¬ë§¤ì˜í–¥', ascending=True)
colors = plt.cm.Oranges(np.linspace(0.3, 0.9, len(concern_df)))
bars = ax3.barh(concern_df['ê³ ë¯¼'], concern_df['êµ¬ë§¤ì˜í–¥'], color=colors, edgecolor='black')
ax3.set_xlabel('êµ¬ë§¤ ì˜í–¥ ë¹„ìœ¨', fontsize=12)
ax3.set_title('ë‘í”¼ ê³ ë¯¼ë³„ êµ¬ë§¤ ì˜í–¥', fontsize=14, fontweight='bold')
ax3.set_xlim(0, 1)
for bar, (_, row) in zip(bars, concern_df.iterrows()):
    ax3.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height()/2, 
             f'{row["êµ¬ë§¤ì˜í–¥"]:.0%} (n={int(row["ì‘ë‹µìˆ˜"])})', ha='left', va='center', fontsize=10)

# 4-4: ì„±ë³„ Ã— í•˜ë£¨2ë²ˆìƒ´í‘¸ Ã— êµ¬ë§¤ì˜í–¥
ax4 = axes[1, 1]
cross_data = df.groupby(['ì„±ë³„', 'í•˜ë£¨2ë²ˆìƒ´í‘¸'])['êµ¬ë§¤ì˜í–¥'].mean().unstack()
cross_data.columns = ['í•˜ë£¨ 1ë²ˆ', 'í•˜ë£¨ 2ë²ˆ']
cross_data.plot(kind='bar', ax=ax4, color=['#3498db', '#e74c3c'], edgecolor='black', width=0.7)
ax4.set_xlabel('ì„±ë³„', fontsize=12)
ax4.set_ylabel('êµ¬ë§¤ ì˜í–¥ ë¹„ìœ¨', fontsize=12)
ax4.set_title('ì„±ë³„ Ã— ìƒ´í‘¸ íšŸìˆ˜ë³„ êµ¬ë§¤ ì˜í–¥', fontsize=14, fontweight='bold')
ax4.set_ylim(0, 1)
ax4.legend(title='ìƒ´í‘¸ íšŸìˆ˜')
ax4.set_xticklabels(ax4.get_xticklabels(), rotation=0)
for container in ax4.containers:
    ax4.bar_label(container, fmt='%.0f%%', label_type='edge', fontsize=10)

plt.suptitle('ë°ì´ì•¤ë‚˜ì´íŠ¸ ë“€ì–¼ ìƒ´í‘¸ - ì£¼ìš” ì¸ì‚¬ì´íŠ¸', fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('key_insights.png', dpi=150, bbox_inches='tight', facecolor='white')
print("   âœ… key_insights.png ì €ì¥ ì™„ë£Œ")

# Figure 5: Logistic Regression ê³„ìˆ˜ ì‹œê°í™” (ì˜í–¥ ë°©í–¥ í¬í•¨)
fig5, ax = plt.subplots(figsize=(12, 8))
lr_sorted = lr_coef.sort_values('ê³„ìˆ˜')
colors = ['#2ecc71' if c > 0 else '#e74c3c' for c in lr_sorted['ê³„ìˆ˜']]
bars = ax.barh(lr_sorted['í”¼ì²˜'], lr_sorted['ê³„ìˆ˜'], color=colors, edgecolor='black')
ax.axvline(x=0, color='black', linestyle='-', linewidth=1)
ax.set_xlabel('Logistic Regression ê³„ìˆ˜', fontsize=12)
ax.set_title('êµ¬ë§¤ ì˜í–¥ì— ëŒ€í•œ ê° ë³€ìˆ˜ì˜ ì˜í–¥\n(ğŸŸ¢ ì–‘ì˜ ì˜í–¥: êµ¬ë§¤â†‘ / ğŸ”´ ìŒì˜ ì˜í–¥: êµ¬ë§¤â†“)', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('logistic_regression_coefficients.png', dpi=150, bbox_inches='tight', facecolor='white')
print("   âœ… logistic_regression_coefficients.png ì €ì¥ ì™„ë£Œ")

print("\n" + "=" * 70)
print("âœ… ë¶„ì„ ì™„ë£Œ!")
print("=" * 70)
print("""
ğŸ“ ìƒì„±ëœ íŒŒì¼:
   1. feature_importance_comparison.png - 3ê°œ ëª¨ë¸ Feature Importance ë¹„êµ
   2. decision_tree_visualization.png   - Decision Tree ì‹œê°í™”
   3. model_performance.png             - ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ & ROC Curve
   4. key_insights.png                  - ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ìš”ì•½
   5. logistic_regression_coefficients.png - ë³€ìˆ˜ë³„ ì˜í–¥ ë°©í–¥
""")
